Large-scale helicopters have unique characteristics of maneuverability and low-speed performance compared to fixed-wing aircraft. They can take off and land vertically, hover in place for extended periods of time, and move in all six directions, making them occupy important niches in both military and civil aviation. However, compared to fixed-wing aircraft, these advantages come at a cost: helicopters are inherently unstable with complicated dynamics, and generally less safe than commercial air travel \cite{IHST}. Although this is partly due to being used for risky missions in the first place, loss of control in-flight (LOC-I) is the largest key risk area for helicopter operations \cite{EASA}, and engine failure is the cause of 75.9\% of helicopter accidents in the category systems failure \cite{IHST}. 

 The difficult handling of helicopters combined with their risky flight profiles make them especially good candidates for advanced flight control systems \cite{Hu2017}. Traditionally, flight control systems rely on classical control methods and use gain scheduling to switch between different controllers for each flight regime. Creating these controllers is a labor-intensive, precise task for fixed-wing aircraft, and even more so for the wide variety of flight regimes of a helicopter \cite{Hu2017}. Furthermore, the accompanying strategy for situations that fall outside what the autopilot can handle is to turn the autopilot off and give a warning to the pilot, whose main task is then to monitor the automation system, which is what humans are notoriously bad at \cite{ironies}. An \emph{adaptive flight control} system, one that could react to changing conditions, would therefore be the next logical step. Techniques such as nonlinear dynamic inversion and backstepping have successfully been applied to deal with system non-linearities \cite{NDI, BS}. However, these techniques require a high quality model of the aircraft throughout its flight envelope. More recently, incremental variations of these techniques (incremental nonlinear dynamic inversion \cite{INDI} and incremental backstepping \cite{IBS}) have been implemented on real fixed-wing aircraft \cite{Pollack2019, Keijzer2019} as well as high-fidelity models of rotorcraft \cite{Simplicio2013}, and have shown promising results and a reduced model dependency. However, in return for that reduced model dependancy, these methods require fast and accurate acceleration measurements, and while these methods have improved fault tolerance over traditional methods, they still require aircraft models a priori. 

Another promising avenue of adaptive flight control seeks to use Reinforcement Learning (RL), a field of machine learning where agents learn what actions to take by interacting with the environment. The agent is not told explicitly what good and bad actions are, but must discover this themselves by means of trial and error \cite{book:suttonbarto}. Good results are reinforced by numerical rewards. This has as an advantage that a policy can be learned online and purely from experience, without any knowledge of plant dynamics. Traditionally, RL was only formulated for discrete state and action spaces, where results could be kept in a tabular format. With the introduction of function approximators, RL methods called Adaptive Critic Designs (ACDs) have been successfully applied for adaptive flight control of missiles \cite{Bertsekas2000}, helicopters \cite{Enns2002, Enns2003a, Enns2003b}, business jets \cite{Ferrari2004} and military aircraft \cite{VanKampen2006}. However, these methods often need hundreds to thousands of offline training episodes, both to approximate a global nonlinear system model as well as to train the controllers themselves, which requires a high-quality simulation model of the controlled system \cite{Prokhorov1995, Balakrishnan1996, Prokhorov1997}. 

It becomes clear that neither incremental control techniques nor traditional ACDs will lead to true model-free control. Based on a synthesis between the advancements in incremental model techniques and RL, two novel frameworks called Incremental Heuristic Dynamic Programming (IHDP) \cite{Zhou2016HDP} and Incremental Dual Heuristic Programming (IDHP) \cite{Zhou2018DHP} have been proposed. These frameworks learn an incremental model in real time and therefore do not require an offline learning phase. The feasibility of this approach was demonstrated in \cite{Heyer2020}, where it was shown that the IDHP framework could be used for near-optimal control of a CS-25 class fixed-wing research aircraft without prior knowledge of the system dynamics or an offline learning phase. Compared to small, fixed-wing aircraft in cruise, rotorcraft have relatively slow control responses and are unstable or marginally stable in almost all flight regimes. One design choice in \cite{Heyer2020} traded speed of convergence away for increased learning stability. In online adaptive control of rotorcraft, this trade-off is non-trivial, as there is the possibility of the system itself diverging before the controller has learned to control it. 

The focus of this paper is to analyze the applicability of the IDHP framework for online adaptive control of a nonlinear, six-degree-of-freedom simulation model of a MBB Bo-105 helicopter. To reduce the scope of the research, only the collective and longitudinal cyclic were controlled by RL agents. A control system is proposed containing two separate IDHP agents to control the collective and longitudinal cyclic, directly tracking a reference altitude and pitch angle, respectively. Outer loop control and the lateral motions are controlled by conventional PID controllers. 

The remainder of this paper is structured as follows. Section \ref{sec:framework} explains the working principles behind basic RL and follows up with the IDHP algorithm. In Section \ref{sec:controller}, the simulation model is discussed and the integration of IDHP in a complete control system is shown. Next, Section \ref{sec:experiment} describes the experiments performed to find the optimal hyperparameters for this set-up and to test the performance of the resulting control system. The results of these experiments are discussed in Section \ref{sec:results}. Finally, Section \ref{sec:conclusions} concludes the paper. 